{
  "cells": [
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Instanzen und Erweiterungen"
      ]
    },
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "In diesem Kapitel werden mehrere Quantenvariationsalgorithmen behandelt, darunter:\n",
        "\n",
        "- [Variationeller Quanteneigenlöser (VQE)](https://arxiv.org/abs/1304.3061)\n",
        "- [Subraumsuche VQE (SSVQE)](https://arxiv.org/abs/1810.09434)\n",
        "- [Variationelle Quantendeflation (VQD)](https://arxiv.org/abs/1805.08138)\n",
        "- [Quanten-Sampling-Regression (QSR)](https://arxiv.org/pdf/2012.02338)\n",
        "\n",
        "Durch die Verwendung dieser Algorithmen lernen wir verschiedene Designideen kennen, die in benutzerdefinierte Variationsalgorithmen integriert werden können, z. B. Gewichtungen, Strafen, Überabtastung und Unterabtastung. Wir ermutigen Sie, mit diesen Konzepten zu experimentieren und Ihre Erkenntnisse mit der Community zu teilen."
      ]
    },
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Variationeller Quanteneigenlöser (VQE)\n",
        "\n",
        "[VQE](https://arxiv.org/abs/1304.3061) ist einer der am weitesten verbreiteten Variationsquantenalgorithmen und stellt eine Vorlage dar, auf der andere Algorithmen aufbauen können.\n",
        "\n",
        "![VQE](images/instances_VQE.png)"
      ]
    },
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Das Layout von VQE ist einfach:\n",
        "\n",
        "- Bereiten Sie die Referenzoperatoren $U_R$ vor\n",
        "    - Wir beginnen mit dem Zustand $|0\\rangle$ und gehen zum Referenzzustand $|\\rho\\rangle$ über\n",
        "- Wenden Sie die Variationsform $U_V(\\vec\\theta_{i,j})$ an, um einen Ansatz $U_A(\\vec\\theta_{i,j})$ zu erstellen\n",
        "    - Wir gehen vom Zustand $|\\rho\\rangle$ zu $U_V(\\vec\\theta_{i,j})|\\rho\\rangle = |\\psi(\\vec\\theta_{i,j})\\rangle$\n",
        "- Bootstrap bei $i=0$, wenn wir ein ähnliches Problem haben (normalerweise durch klassische Simulation oder Stichprobe gefunden)\n",
        "    - Jeder Optimierer wird anders gebootet, was zu einem anfänglichen Satz von Parametervektoren $\\Theta_0 := \\{ {\\vec\\theta_{0,j} | führt j \\in \\mathcal{J}_\\text{opt}^0} \\}$ (z. B. von einem Anfangspunkt $\\vec\\theta_0$).\n",
        "- Bewerten Sie die Kostenfunktion $C(\\vec\\theta_{i,j}) := \\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle$ für alle vorbereiteten Zustände auf einem Quantencomputer.\n",
        "- Verwenden Sie einen klassischen Optimierer, um den nächsten Parametersatz $\\Theta_{i+1}$ auszuwählen.\n",
        "- Wiederholen Sie den Vorgang, bis die Konvergenz erreicht ist.\n",
        "\n",
        "Dies ist eine einfache klassische Optimierungsschleife, in der wir die Kostenfunktion auswerten. Einige Optimierer erfordern möglicherweise mehrere Auswertungen, um einen Gradienten zu berechnen, die nächste Iteration zu bestimmen oder die Konvergenz zu bewerten.\n",
        "\n",
        "Hier ist das Beispiel für die folgende Observable:\n",
        "\n",
        "$$\n",
        "\\hat{O}_1 = 2 II - 2 XX + 3 YY - 3 ZZ,\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "uses-hardware"
        ]
      },
      "outputs": [

      ],
      "source": [
        "from qiskit.circuit.library import TwoLocal\n",
        "from qiskit.quantum_info import SparsePauliOp\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit_ibm_runtime import QiskitRuntimeService, Estimator\n",
        "import numpy as np\n",
        "\n",
        "# Add your token below\n",
        "service = QiskitRuntimeService(\n",
        "    channel=\"ibm_quantum\",\n",
        ")\n",
        "\n",
        "def cost_function_vqe(theta):\n",
        "    observable = SparsePauliOp.from_list([(\"II\", 2), (\"XX\", -2), (\"YY\", 3), (\"ZZ\", -3)])\n",
        "    reference_circuit = QuantumCircuit(2)\n",
        "    reference_circuit.x(0)\n",
        "\n",
        "    variational_form = TwoLocal(\n",
        "        2,\n",
        "        rotation_blocks=[\"rz\", \"ry\"],\n",
        "        entanglement_blocks=\"cx\",\n",
        "        entanglement=\"linear\",\n",
        "        reps=1,\n",
        "    )\n",
        "    ansatz = reference_circuit.compose(variational_form)\n",
        "\n",
        "    backend = service.backend(\"ibmq_qasm_simulator\")\n",
        "    \n",
        "    # Use estimator to get the expected values corresponding to each ansatz\n",
        "    estimator = Estimator(session=backend)\n",
        "    job = estimator.run(ansatz, observable, theta)\n",
        "    values = job.result().values\n",
        "\n",
        "    return values"
      ]
    },
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Mit dieser Kostenfunktion können wir optimale Parameter berechnen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [

      ],
      "source": [
        "from qiskit.algorithms.optimizers import COBYLA\n",
        "\n",
        "initial_theta = np.ones(8)\n",
        "optimizer = COBYLA()\n",
        "\n",
        "optimizer_result = optimizer.minimize(fun=cost_function_vqe, x0=initial_theta)\n",
        "\n",
        "optimal_parameters = optimizer_result.x\n",
        "print(optimal_parameters)"
      ]
    },
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Schließlich können wir unsere optimalen Parameter verwenden, um unsere minimalen Eigenwerte zu berechnen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [

      ],
      "source": [
        "observable = SparsePauliOp.from_list([(\"II\", 2), (\"XX\", -2), (\"YY\", 3), (\"ZZ\", -3)])\n",
        "reference_circuit = QuantumCircuit(2)\n",
        "reference_circuit.x(0)\n",
        "\n",
        "variational_form = TwoLocal(\n",
        "    2,\n",
        "    rotation_blocks=[\"rz\", \"ry\"],\n",
        "    entanglement_blocks=\"cx\",\n",
        "    entanglement=\"linear\",\n",
        "    reps=1,\n",
        ")\n",
        "ansatz = reference_circuit.compose(variational_form)\n",
        "solution = ansatz.bind_parameters(optimal_parameters)\n",
        "\n",
        "backend = service.backend(\"ibmq_qasm_simulator\")\n",
        "estimator = Estimator(session=backend)\n",
        "job = estimator.run(solution, observable)\n",
        "values = job.result().values\n",
        "\n",
        "experimental_min_eigenvalue = values[0]\n",
        "print(experimental_min_eigenvalue)"
      ]
    },
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Subraumsuche VQE (SSVQE)\n",
        "\n",
        "[SSVQE](https://arxiv.org/abs/1810.09434) ist eine Variante von VQE, die es ermöglicht, die ersten $k$ Eigenwerte einer Observablen $\\hat{H}$ mit den Eigenwerten {$\\lambda_0, \\lambda_1,...,\\lambda_{N-1}$} zu erhalten, wobei $N\\geq k$. Ohne Beschränkung der Allgemeinheit gehen wir davon aus, dass $\\lambda_0&lt;\\lambda_1&lt;...&lt;\\lambda_{N-1}$. SSQVE führt eine neue Idee ein, indem Gewichtungen hinzugefügt werden, um die Optimierung für den Term mit der größten Gewichtung zu priorisieren.\n",
        "\n",
        "![SSVQE](images/instances_SSVQE.png)\n",
        "\n",
        "Um diesen Algorithmus zu implementieren, benötigen wir $k$ zueinander orthogonale Referenzzustände `{latex} \\{ |\\rho_j\\rangle \\}_{j=0}^{k-1}` , also $\\langle \\rho_j | \\rho_l \\rangle = \\delta_{jl}$ für $j,l&lt;k$. Diese Zustände können mit Pauli-Operatoren konstruiert werden. Die Kostenfunktion dieses Algorithmus lautet dann:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "C(\\vec{\\theta}) \n",
        "\n",
        "& := \\sum_{j=0}^{k-1} w_j \\langle \\rho_j | U_{V}^{\\dagger}(\\vec{\\theta})\\hat{H} U_{V}(\\vec{\\theta})|\\rho_j \\rangle \\\\[1mm]\n",
        "\n",
        "& := \\sum_{j=0}^{k-1} w_j \\langle \\psi_{j}(\\vec{\\theta}) | \\hat{H} | \\psi_{j}(\\vec{\\theta}) \\rangle \\\\[1mm]\n",
        "\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "wobei $w_j$ eine beliebige positive Zahl ist, so dass, wenn $j&lt;l&lt;k$, dann $w_j&gt;w_l$ ist und $U_V(\\vec{\\theta})$ die benutzerdefinierte Variationsform ist.\n",
        "\n",
        "Der SSVQE-Algorithmus basiert auf der Tatsache, dass Eigenzustände, die unterschiedlichen Eigenwerten entsprechen, zueinander orthogonal sind. Insbesondere kann das innere Produkt von $U_V(\\vec{\\theta})|\\rho_j\\rangle$ und $U_V(\\vec{\\theta})|\\rho_l\\rangle$ ausgedrückt werden als:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\langle \\rho_j | U_{V}^{\\dagger}(\\vec{\\theta})U_{V}(\\vec{\\theta})|\\rho_l \\rangle\n",
        "\n",
        "& = \\langle \\rho_j | I |\\rho_l \\rangle \\\\[1mm]\n",
        "\n",
        "& = \\langle \\rho_j | \\rho_l \\rangle \\\\[1mm]\n",
        "\n",
        "& = \\delta_{jl}\n",
        "\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Die erste Gleichung gilt, weil $U_{V}(\\vec{\\theta})$ ein Quantenoperator und daher einheitlich ist. Die letzte Gleichheit gilt aufgrund der Orthogonalität der Referenzzustände $|\\rho_j\\rangle$. Die Tatsache, dass die Orthogonalität durch einheitliche Transformationen erhalten bleibt, hängt eng mit dem Prinzip der Informationserhaltung zusammen, wie es in der Quanteninformationswissenschaft zum Ausdruck kommt. Aus dieser Sicht stellen nicht-unitäre Transformationen Prozesse dar, bei denen Informationen entweder verloren gehen oder eingefügt werden.\n",
        "\n",
        "Gewichte $w_j$ tragen dazu bei, dass alle Zustände Eigenzustände sind. Wenn die Gewichte ausreichend unterschiedlich sind, erhält der Term mit dem größten Gewicht (d. h. $w_0$) bei der Optimierung Vorrang vor den anderen. Infolgedessen wird der resultierende Zustand $U_{V}(\\vec{\\theta})|\\rho_0 \\rangle$ zum Eigenzustand, der $\\lambda_0$ entspricht. `{latex} \\{ U_{V}(\\vec{\\theta})|\\rho_j\\rangle \\}_{j=0}^{k-1}` , also im Unterraum enthalten, der den Eigenwerten {$\\lambda_1,...,\\lambda_{N-1}$} entspricht.\n",
        "\n",
        "Wenn man das gleiche Argument auf den Rest der Terme anwendet, wäre die nächste Priorität dann der Term mit der Gewichtung $w_1$, also wäre $U_{V}(\\vec{\\theta})|\\rho_1 \\rangle$ der entsprechende Eigenzustand zu $\\lambda_1$, und die anderen Terme wären im Eigenraum von {$\\lambda_2,...,\\lambda_{N-1}$} enthalten.\n",
        "\n",
        "Durch induktives Denken folgern wir, dass $U_{V}(\\vec{\\theta})|\\rho_j \\rangle$ ein ungefährer Eigenzustand von $\\lambda_j$ für $0\\leq j &lt; k$ sein wird.\n",
        "\n",
        "SSVQE's können wie folgt zusammengefasst werden:\n",
        "\n",
        "- Bereiten Sie mehrere Referenzzustände vor, indem Sie ein einheitliches U_R auf k verschiedene rechnerische Basiszustände anwenden\n",
        "    - Dieser Algorithmus erfordert die Verwendung von $k$ zueinander orthogonalen Referenzzuständen `{latex} \\{ |\\rho_j\\rangle \\}_{j=0}^{k-1}` , so dass $\\langle \\rho_j | \\rho_l \\rangle = \\delta_{jl}$ für $j,l&lt;k$.\n",
        "- Wenden Sie die Variationsform $U_V(\\vec\\theta_{i,j})$ auf jeden Referenzzustand an, was den folgenden Ansatz $U_A(\\vec\\theta_{i,j})$ ergibt.\n",
        "- Bootstrap bei $i=0$, wenn ein ähnliches Problem verfügbar ist (normalerweise durch klassische Simulation oder Stichprobe gefunden).\n",
        "- Bewerten Sie die Kostenfunktion $C(\\vec\\theta_{i,j}) := \\sum_{j=0}^{k-1} w_j \\langle \\psi_{j}(\\vec{\\theta}) | \\hat{H} | \\psi_{j}(\\vec{\\theta}) \\rangle$ für alle vorbereiteten Zustände auf einem Quantencomputer.\n",
        "    - Dies kann in die Berechnung des Erwartungswerts für ein beobachtbares $\\langle \\psi_{j}(\\vec{\\theta}) | unterteilt werden \\hat{H} | \\psi_{j}(\\vec{\\theta}) \\rangle$ und Multiplikation dieses Ergebnisses mit $w_j$.\n",
        "    - Anschließend gibt die Kostenfunktion die Summe aller gewichteten Erwartungswerte zurück.\n",
        "- Verwenden Sie einen klassischen Optimierer, um den nächsten Parametersatz $\\Theta_{i+1}$ zu bestimmen.\n",
        "- Wiederholen Sie die oben genannten Schritte, bis die Konvergenz erreicht ist.\n",
        "\n",
        "Sie werden die Kostenfunktion von SSVQE in der Bewertung rekonstruieren!"
      ]
    },
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Variationelle Quantendeflation (VQD)\n",
        "\n",
        "[VQD](https://arxiv.org/abs/1805.08138) ist eine iterative Methode, die VQE erweitert, um die ersten $k$ Eigenwerte einer Observablen $\\hat{H}$ mit den Eigenwerten {$\\lambda_0, \\lambda_1,...,\\lambda_{N-1}$} zu erhalten, wobei $N\\geq k$, statt nur das erste. Für den Rest dieses Abschnitts gehen wir ohne Beschränkung der Allgemeinheit davon aus, dass $\\lambda_0\\leq\\lambda_1\\leq...\\leq\\lambda_{N-1}$. VQD führt den Begriff der Strafkosten ein, um den Optimierungsprozess zu steuern.\n",
        "\n",
        "![VQD](images/instances_VQD.png)"
      ]
    },
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "VQD führt einen Strafterm ein, der als $\\beta$ bezeichnet wird, um den Beitrag jedes Überlappungsterms zu den Kosten auszugleichen. Dieser Strafterm dient dazu, den Optimierungsprozess zu bestrafen, wenn die Orthogonalität nicht erreicht wird. Wir legen diese Einschränkung fest, weil die Eigenzustände einer Observablen oder eines hermiteschen Operators, die verschiedenen Eigenwerten entsprechen, immer zueinander orthogonal sind oder im Fall von Entartung oder wiederholten Eigenwerten so gemacht werden können. Indem wir also die Orthogonalität mit dem Eigenzustand erzwingen, der $\\lambda_0$ entspricht, optimieren wir effektiv über den Unterraum, der den restlichen Eigenwerten {$\\lambda_1, \\lambda_2,..., \\lambda_{N-1}$ entspricht }. Hier ist $\\lambda_1$ der niedrigste Eigenwert unter den übrigen Eigenwerten und daher kann die optimale Lösung des neuen Problems mithilfe des Variationssatzes erhalten werden.\n",
        "\n",
        "Die allgemeine Idee hinter VQD besteht darin, VQE wie üblich zu verwenden, um den niedrigsten Eigenwert $\\lambda_0 := C_0(\\vec\\theta^0) \\equiv C_\\text{VQE}(\\vec\\theta^0)$ zusammen mit dem zu erhalten entsprechender (ungefährer) Eigenzustand $|\\psi(\\vec{\\theta^0})\\rangle$ für einen optimalen Parametervektor $\\vec{\\theta^0}$. Um dann den nächsten Eigenwert $\\lambda_1 &gt; \\lambda_0$ zu erhalten, muss die Kostenfunktion $C_0(\\vec{\\theta}) := \\langle \\psi(\\vec{\\theta}) | minimiert werden \\hat{H} | \\psi(\\vec{\\theta})\\rangle$, wir optimieren:\n",
        "\n",
        "$$\n",
        "C_1(\\vec{\\theta}) := \n",
        "C_0(\\vec{\\theta})+ \\beta_0 |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^0})\\rangle  |^2 \n",
        "$$\n",
        "\n",
        "Der positive Wert $\\beta_0$ sollte idealerweise größer als $\\lambda_1-\\lambda_0$ sein.\n",
        "\n",
        "Dies führt eine neue Kostenfunktion ein, die als eingeschränktes Problem betrachtet werden kann, wobei wir $C_\\text{VQE}(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | minimieren \\hat{H} | \\psi(\\vec{\\theta})\\rangle$ unterliegt der Einschränkung, dass der Zustand orthogonal zum zuvor erhaltenen $|\\psi(\\vec{\\theta^0})\\rangle$ sein muss, mit $\\beta_0$ fungiert als Strafterm, wenn die Einschränkung nicht erfüllt ist.\n",
        "\n",
        "Alternativ kann dieses neue Problem so interpretiert werden, dass VQE auf dem neuen Observable ausgeführt wird:\n",
        "\n",
        "$$\n",
        "\\hat{H_1} := \\hat{H} + \\beta_0 |\\psi(\\vec{\\theta^0})\\rangle \\langle \\psi(\\vec{\\theta^0})|\n",
        "\\quad \\Rightarrow \\quad \n",
        "C_1(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H_1} | \\psi(\\vec{\\theta})\\rangle,\n",
        "$$\n",
        "\n",
        "Unter der Annahme, dass die Lösung des neuen Problems $|\\psi(\\vec{\\theta^1})\\rangle$ ist, sollte der erwartete Wert von $\\hat{H}$ (nicht $\\hat{H_1}$) sein $ \\langle \\psi(\\vec{\\theta^1}) | \\hat{H} | \\psi(\\vec{\\theta^1})\\rangle = \\lambda_1$."
      ]
    },
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Um den dritten Eigenwert $\\lambda_2$ zu erhalten, lautet die zu optimierende Kostenfunktion:\n",
        "\n",
        "$$\n",
        "C_2(\\vec{\\theta}) := \n",
        "C_1(\\vec{\\theta}) + \\beta_1 |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^1})\\rangle  |^2 \n",
        "$$\n",
        "\n",
        "wobei $\\beta_1$ eine positive Konstante ist, die groß genug ist, um die Orthogonalität des Lösungszustands sowohl zu $|\\psi(\\vec{\\theta^0})\\rangle$ als auch zu $|\\psi(\\vec{\\theta^1) zu erzwingen })\\rangle$. Dadurch werden Staaten im Suchraum bestraft, die diese Anforderung nicht erfüllen, wodurch der Suchraum effektiv eingeschränkt wird. Daher sollte die optimale Lösung des neuen Problems der Eigenzustand sein, der $\\lambda_2$ entspricht.\n",
        "\n",
        "Wie der vorherige Fall kann auch dieses neue Problem als VQE mit der Observablen interpretiert werden:\n",
        "\n",
        "$$\n",
        "\\hat{H_2} := \\hat{H_1} + \\beta_1 |\\psi(\\vec{\\theta^1})\\rangle \\langle \\psi(\\vec{\\theta^1})|\n",
        "\\quad \\Rightarrow \\quad \n",
        "C_2(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H_2} | \\psi(\\vec{\\theta})\\rangle.\n",
        "$$\n",
        "\n",
        "Wenn die Lösung für dieses neue Problem $|\\psi(\\vec{\\theta^2})\\rangle$ lautet, sollte der erwartete Wert von $\\hat{H}$ (nicht $\\hat{H_2}$) $ sein \\langle \\psi(\\vec{\\theta^2}) | \\hat{H} | \\psi(\\vec{\\theta^2})\\rangle = \\lambda_2$."
      ]
    },
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Um den $k$-ten Eigenwert $\\lambda_{k-1}$ zu erhalten, würden Sie analog die Kostenfunktion minimieren:\n",
        "\n",
        "$$\n",
        "C_{k-1}(\\vec{\\theta}) := \n",
        "C_{k-2}(\\vec{\\theta}) + \\beta_{k-2} |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^{k-2}})\\rangle  |^2,\n",
        "$$\n",
        "\n",
        "Denken Sie daran, dass wir $\\vec{\\theta^j}$ so definiert haben, dass $\\langle \\psi(\\vec{\\theta^j}) | \\hat{H} | \\psi(\\vec{\\theta^j})\\rangle = \\lambda_j, \\forall j&lt;k$. Dieses Problem entspricht der Minimierung von $C(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle$, aber mit der Einschränkung, dass der Zustand orthogonal zu $|\\psi(\\vec{\\theta^j})\\rangle sein muss; \\forall j \\in {0, \\cdots, k-1}$, wodurch der Suchraum auf den Unterraum beschränkt wird, der den Eigenwerten {$\\lambda_{k-1},\\cdots,\\lambda_{N-1}$ entspricht }.\n",
        "\n",
        "Dieses Problem entspricht einem VQE mit der Observablen:\n",
        "\n",
        "$$\n",
        "\\hat{H}_{k-1} := \n",
        "\\hat{H}_{k-2} + \\beta_{k-2} |\\psi(\\vec{\\theta^{k-2}})\\rangle \\langle \\psi(\\vec{\\theta^{k-2}})|\n",
        "\\quad \\Rightarrow \\quad \n",
        "C_{k-1}(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H}_{k-1} | \\psi(\\vec{\\theta})\\rangle,\n",
        "$$\n",
        "\n",
        "Wie Sie dem Prozess entnehmen können, benötigen Sie zum Erhalten des $k$-ten Eigenwerts die (ungefähren) Eigenzustände der vorherigen $k-1$-Eigenwerte, sodass Sie VQE insgesamt $k$-mal ausführen müssten. Daher lautet die Kostenfunktion von VQD wie folgt:\n",
        "\n",
        "$$\n",
        "C_k(\\vec{\\theta}) =\n",
        "\\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle +\n",
        "\\sum_{j=0}^{k-1}\\beta_j |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^j})\\rangle |^2\n",
        "$$\n",
        "\n",
        "Das Layout von VQD lässt sich wie folgt zusammenfassen:\n",
        "\n",
        "- Bereiten Sie einen Referenzoperator $U_R$ vor\n",
        "- Wenden Sie die Variationsform $U_V(\\vec\\theta_{i,j})$ auf den Referenzzustand an und erstellen Sie den folgenden Ansatz $U_A(\\vec\\theta_{i,j})$\n",
        "- Bootstrap bei $i=0$, wenn wir ein ähnliches Problem haben (normalerweise durch klassische Simulation oder Stichprobe gefunden).\n",
        "- Bewerten Sie die Kostenfunktion $C_k(\\vec{\\theta})$, was die Berechnung von $k$ angeregten Zuständen und einem Array von $\\beta$s umfasst, die die Überlappungsstrafe für jeden Überlappungsterm definieren.\n",
        "    - Berechnen Sie den Erwartungswert für eine Observable $\\langle \\psi_{j}(\\vec{\\theta}) | \\hat{H} | \\psi_{j}(\\vec{\\theta}) \\rangle$ für jedes $k$\n",
        "    - Berechnen Sie die Strafe $\\sum_{j=0}^{k-1}\\beta_j |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^j})\\rangle |^2$.\n",
        "    - Die Kostenfunktion sollte dann die Summe dieser beiden Terme zurückgeben\n",
        "- Verwenden Sie einen klassischen Optimierer, um den nächsten Parametersatz $\\Theta_{i+1}$ auszuwählen.\n",
        "- Wiederholen Sie diesen Vorgang, bis die Konvergenz erreicht ist."
      ]
    },
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Quanten-Sampling-Regression (QSR)\n",
        "\n",
        "Eines der Hauptprobleme bei VQE sind die mehrfachen Aufrufe an einen Quantencomputer, die erforderlich sind, um die Parameter für jeden Schritt zu erhalten, einschließlich $k$, $k-1$ usw. Dies ist besonders problematisch, wenn der Zugriff auf Quantengeräte in der Warteschlange steht . Während eine [`Session`](https://qiskit.org/documentation/partners/qiskit_ibm_runtime/how_to/run_session.html) zum Gruppieren mehrerer iterativer Aufrufe verwendet werden kann, besteht ein alternativer Ansatz in der Verwendung von Stichproben. Durch den Einsatz klassischerer Ressourcen können wir den gesamten Optimierungsprozess in einem einzigen Anruf abschließen. Hier kommt [die Quantum Sampling Regression](https://arxiv.org/pdf/2012.02338) ins Spiel. Da der Zugang zu Quantencomputern immer noch ein Gut mit geringem Angebot und hoher Nachfrage ist, halten wir diesen Kompromiss für viele aktuelle Studien für möglich und praktisch. Dieser Ansatz nutzt alle verfügbaren klassischen Fähigkeiten und erfasst gleichzeitig viele der inneren Abläufe und intrinsischen Eigenschaften von Quantenberechnungen, die in der Simulation nicht auftauchen.\n",
        "\n",
        "![QSR](images/instances_QSR.png)\n",
        "\n",
        "Die Idee hinter QSR ist, dass die Kostenfunktion $C(\\theta) := \\langle \\psi(\\theta) | \\hat{H} | \\psi(\\theta)\\rangle$ kann auf folgende Weise als Fourier-Reihe ausgedrückt werden:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "C(\\vec{\\theta}) \n",
        "\n",
        "& := \\langle \\psi(\\theta) | \\hat{H} | \\psi(\\theta)\\rangle \\\\[1mm]\n",
        "\n",
        "& := a_0 + \\sum_{k=1}^S[a_k\\cos(k\\theta)+ b_k\\sin(k\\theta)] \\\\[1mm]\n",
        "\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Abhängig von der Periodizität und Bandbreite der ursprünglichen Funktion kann die Menge $S$ endlich oder unendlich sein. Für die Zwecke dieser Diskussion gehen wir davon aus, dass es unendlich ist. Der nächste Schritt besteht darin, die Kostenfunktion $C(\\theta)$ mehrmals abzutasten, um die Fourier-Koeffizienten ${a_0, a_k, b_k}_{k=1}^S$ zu erhalten. Da wir insbesondere $2S+1$ Unbekannte haben, müssen wir die Kostenfunktion $2S+1$ mal abtasten.\n",
        "\n",
        "Wenn wir dann die Kostenfunktion für $2S+1$-Parameterwerte {$\\theta_1,...,\\theta_{2S+1}$} abtasten, können wir das folgende System erhalten:\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix} 1 & \\cos(\\theta_1) & \\sin(\\theta_1) & \\cos(2\\theta_1) & ... & \\sin(S\\theta_1) \\\\\n",
        "1 & \\cos(\\theta_2) & \\sin(\\theta_2) & \\cos(2\\theta_2) & \\cdots & \\sin(S\\theta_2)\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
        "1 & \\cos(\\theta_{2S+1}) & \\sin(\\theta_{2S+1}) & \\cos(2\\theta_{2S+1}) & \\cdots & \\sin(S\\theta_{2S+1})\n",
        "\\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_1 \\\\ b_1 \\\\ a_2 \\\\ \\vdots \\\\ b_S \\end{pmatrix} = \\begin{pmatrix} C(\\theta_1) \\\\ C(\\theta_2) \\\\ \\vdots \\\\ C(\\theta_{2S+1}) \\end{pmatrix},\n",
        "$$\n",
        "\n",
        "dass wir umschreiben werden als\n",
        "\n",
        "$$\n",
        "Fa=c.\n",
        "$$\n",
        "\n",
        "In der Praxis ist dieses System im Allgemeinen nicht konsistent, da die Kostenfunktionswerte $c$ nicht exakt sind. Daher ist es normalerweise eine gute Idee, sie zu normalisieren, indem man sie links mit $F^\\dagger$ multipliziert, was zu Folgendem führt:\n",
        "\n",
        "$$\n",
        "F^\\dagger Fa = F^\\dagger c.\n",
        "$$\n",
        "\n",
        "Dieses neue System ist immer konsistent und seine Lösung ist eine Lösung der kleinsten Quadrate für das ursprüngliche Problem. Wenn wir $k$-Parameter anstelle von nur einem haben und jeder Parameter $\\theta^i$ sein eigenes $S_i$ für $i \\in {1,...,k}$ hat, dann ist die Gesamtzahl der erforderlichen Stichproben Ist:\n",
        "\n",
        "$$\n",
        "T=\\prod_{i=1}^k(2S_i+1)\\leq \\prod_{i=1}^k(2S_{max}+1) = (2S_{max}+1)^n,\n",
        "$$\n",
        "\n",
        "wobei $S_{\\max} = \\max_i(S_i)$. Darüber hinaus eröffnet die Anpassung von $S_{\\max}$ als einstellbarem Parameter (anstatt ihn abzuleiten) neue Möglichkeiten, wie zum Beispiel:\n",
        "\n",
        "- **Oversampling** : zur Verbesserung der Genauigkeit.\n",
        "- **Unterabtastung** : Steigerung der Leistung durch Reduzierung des Laufzeit-Overheads oder Eliminierung lokaler Minima.\n",
        "\n",
        "Das Layout von QSR lässt sich wie folgt zusammenfassen:\n",
        "\n",
        "- Bereiten Sie die Referenzoperatoren $U_R$ vor\n",
        "    - Wir gehen vom Zustand $|0\\rangle$ zum Referenzzustand $|\\rho\\rangle$ über\n",
        "- Wenden Sie die Variationsform $U_V(\\vec\\theta_{i,j})$ an, um einen Ansatz $U_A(\\vec\\theta_{i,j})$ zu erstellen\n",
        "    - Bestimmen Sie die Bandbreite, die jedem Parameter im Ansatz zugeordnet ist. Eine Obergrenze ist ausreichend.\n",
        "- Bootstrap bei $i=0$, wenn wir ein ähnliches Problem haben (normalerweise durch klassische Simulation oder Stichprobe gefunden)\n",
        "- Probieren Sie die Kostenfunktion $C(\\vec\\theta) := a_0 + \\sum_{k=1}^S[a_k\\cos(k\\theta)+ b_k\\sin(k\\theta)]$ mindestens $T$ aus mal\n",
        "    - $T=\\prod_{i=1}^k(2S_i+1)\\leq \\prod_{i=1}^k(2S_{max}+1) = (2S_{max}+1)^n$\n",
        "    - Entscheiden Sie, ob eine Über- oder Unterabtastung durchgeführt werden soll, um Geschwindigkeit und Genauigkeit auszugleichen, indem Sie $T$ anpassen.\n",
        "- Berechnen Sie die Fourier-Koeffizienten aus den Stichproben (dh lösen Sie das normalisierte lineare Gleichungssystem).\n",
        "- Lösen Sie das globale Minimum der resultierenden Regressionsfunktion auf einer klassischen Maschine auf."
      ]
    },
    {
      "attachments": {
      },
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "In dieser Lektion haben Sie die verschiedenen verfügbaren Variationsinstanzen kennengelernt:\n",
        "\n",
        "- Hauptgestaltung\n",
        "- Einführung von Gewichten und Strafen zur Anpassung einer Kostenfunktion\n",
        "- Untersuchung von Unterabtastung und Überabtastung, um einen Kompromiss zwischen Geschwindigkeit und Genauigkeit zu finden\n",
        "\n",
        "Diese Ideen können angepasst werden, um einen benutzerdefinierten Variationsalgorithmus zu bilden, der zu Ihrem Problem passt. Wir ermutigen Sie, Ihre Ergebnisse mit der Community zu teilen. In der nächsten Lektion erfahren Sie, wie Sie einen Variationsalgorithmus zum Lösen einer Anwendung verwenden."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
